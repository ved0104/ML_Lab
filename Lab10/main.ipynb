{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5ce789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "572e31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, average='macro'):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    per_class = {}\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for lab in labels:\n",
    "        tp = np.sum((y_true == lab) & (y_pred == lab))\n",
    "        fp = np.sum((y_true != lab) & (y_pred == lab))\n",
    "        fn = np.sum((y_true == lab) & (y_pred != lab))\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        per_class[lab] = (prec, rec, f1)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "    macro_p = np.mean(precisions)\n",
    "    macro_r = np.mean(recalls)\n",
    "    macro_f1 = np.mean(f1s)\n",
    "    return {\n",
    "        'per_class': per_class,\n",
    "        'macro_precision': macro_p,\n",
    "        'macro_recall': macro_r,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed458b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred, name=\"Model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pr = precision_recall_f1(y_true, y_pred)\n",
    "    print(f\"--- {name} Metrics ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Macro Precision: {pr['macro_precision']:.4f}\")\n",
    "    print(f\"Macro Recall: {pr['macro_recall']:.4f}\")\n",
    "    print(f\"Macro F1: {pr['macro_f1']:.4f}\")\n",
    "    print(\"Per-class (precision, recall, f1):\")\n",
    "    for k, v in sorted(pr['per_class'].items()):\n",
    "        print(f\"  {k}: {v[0]:.4f}, {v[1]:.4f}, {v[2]:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93fa7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    if 'label' in df.columns:\n",
    "        # Training data\n",
    "        y = df['label'].values\n",
    "        X = df.drop(columns=['label']).values\n",
    "    else:\n",
    "        # Test data (no labels)\n",
    "        y = np.zeros(df.shape[0], dtype=int)  # dummy placeholder\n",
    "        X = df.values\n",
    "\n",
    "    print(f\"Loaded {path}: X.shape={X.shape}, y.shape={y.shape}\")\n",
    "    return X.astype(np.float32), y.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1445d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    y = np.array(y, dtype=int)\n",
    "    oh = np.zeros((y.shape[0], num_classes), dtype=float)\n",
    "    oh[np.arange(y.shape[0]), y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)  # stability\n",
    "    expz = np.exp(z)\n",
    "    return expz / np.sum(expz, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(probs, y_onehot):\n",
    "    # average negative log-likelihood\n",
    "    eps = 1e-12\n",
    "    logp = np.log(probs + eps)\n",
    "    return -np.mean(np.sum(y_onehot * logp, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4435e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, num_features, num_classes, lr=0.1, reg=0.0):\n",
    "        self.W = np.zeros((num_features, num_classes), dtype=float)  # shape D x C\n",
    "        self.b = np.zeros((1, num_classes), dtype=float)\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        logits = X.dot(self.W) + self.b  # N x C\n",
    "        return softmax(logits)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def fit(self, X, y, epochs=100, batch_size=None, verbose=True):\n",
    "        N, D = X.shape\n",
    "        C = np.max(y) + 1\n",
    "        y_oh = one_hot(y, C)\n",
    "        if batch_size is None:\n",
    "            batch_size = N  # full batch gradient descent\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # simple batching (no shuffling for determinism)\n",
    "            perm = np.arange(N)\n",
    "            np.random.shuffle(perm)\n",
    "            for i in range(0, N, batch_size):\n",
    "                batch_idx = perm[i:i+batch_size]\n",
    "                Xb = X[batch_idx]\n",
    "                yb = y_oh[batch_idx]\n",
    "                probs = self.predict_proba(Xb)\n",
    "                # gradient of loss wrt logits: (probs - y)\n",
    "                grad_logits = (probs - yb) / Xb.shape[0]  # B x C\n",
    "                gradW = Xb.T.dot(grad_logits) + self.reg * self.W\n",
    "                gradb = np.sum(grad_logits, axis=0, keepdims=True)\n",
    "                # update\n",
    "                self.W -= self.lr * gradW\n",
    "                self.b -= self.lr * gradb\n",
    "            if verbose and (epoch % max(1, epochs//10) == 0 or epoch==1):\n",
    "                train_loss = cross_entropy_loss(self.predict_proba(X), y_oh)\n",
    "                train_acc = accuracy_score(y, self.predict(X))\n",
    "                print(f\"Epoch {epoch}/{epochs} - loss: {train_loss:.4f} - acc: {train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07c076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_image(arr, normalize=True, size=(28,28)):\n",
    "    a = np.array(arr).reshape(size)\n",
    "    if normalize:\n",
    "        # scale to 0-255\n",
    "        amin, amax = a.min(), a.max()\n",
    "        if amax > 1.0:\n",
    "            # assume 0-255\n",
    "            norm = np.clip(a, 0, 255).astype(np.uint8)\n",
    "        else:\n",
    "            # assume 0-1\n",
    "            norm = np.clip((a - amin) / (amax - amin + 1e-12) * 255, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        norm = np.clip(a, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f44922c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(sample_array, model, normalize_input=True, show=False):\n",
    "    x = np.array(sample_array).reshape(1, -1).astype(float)\n",
    "    if normalize_input:\n",
    "        # normalize to [0,1]\n",
    "        if x.max() > 1.0:\n",
    "            x = x / 255.0\n",
    "    probs = model.predict_proba(x)\n",
    "    pred = int(np.argmax(probs))\n",
    "    img = array_to_image(x.reshape(-1) if normalize_input else x.reshape(-1), normalize=True)\n",
    "    if show:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"Pred: {pred}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return img, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3fb1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedNaiveBayes:\n",
    "    def __init__(self, categorical_features=None, numeric_features=None, laplace=1.0):\n",
    "        self.cat_feats = categorical_features or []\n",
    "        self.num_feats = numeric_features or []\n",
    "        self.laplace = laplace\n",
    "        self.class_priors = {}\n",
    "        self.cat_cond_probs = {}  # (feat, value, class) -> logprob\n",
    "        self.num_stats = {}  # (feat, class) -> (mean, var)\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X_df, y_series):\n",
    "        df = X_df.copy()\n",
    "        y = np.array(y_series)\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        self.classes_ = classes\n",
    "        total = y.shape[0]\n",
    "        # priors (log)\n",
    "        self.class_priors = {c: np.log(counts[i] / total) for i, c in enumerate(classes)}\n",
    "        # categorical conditional probs\n",
    "        for feat in self.cat_feats:\n",
    "            values = df[feat].astype(str).values\n",
    "            for c in classes:\n",
    "                mask = (y == c)\n",
    "                subset = values[mask]\n",
    "                counter = Counter(subset)\n",
    "            feat_vals = df[feat].astype(str).unique()\n",
    "            V = len(feat_vals)\n",
    "            for c in classes:\n",
    "                mask = (y == c)\n",
    "                subset = df.loc[mask, feat].astype(str).values\n",
    "                cnt = Counter(subset)\n",
    "                denom = len(subset) + self.laplace * V\n",
    "                for v in feat_vals:\n",
    "                    num = cnt.get(v, 0) + self.laplace\n",
    "                    self.cat_cond_probs[(feat, v, c)] = np.log(num / denom)\n",
    "        # numeric stats\n",
    "        for feat in self.num_feats:\n",
    "            for c in classes:\n",
    "                vals = df.loc[y == c, feat].astype(float).values\n",
    "                if vals.size == 0:\n",
    "                    mean = 0.0\n",
    "                    var = 1.0\n",
    "                else:\n",
    "                    mean = np.mean(vals)\n",
    "                    var = np.var(vals) + 1e-9\n",
    "                self.num_stats[(feat, c)] = (mean, var)\n",
    "\n",
    "    def _log_likelihood(self, x_row):\n",
    "        log_probs = {}\n",
    "        for c in self.classes_:\n",
    "            lp = self.class_priors[c]\n",
    "            # categorical\n",
    "            for feat in self.cat_feats:\n",
    "                val = str(x_row[feat])\n",
    "                key = (feat, val, c)\n",
    "                if key in self.cat_cond_probs:\n",
    "                    lp += self.cat_cond_probs[key]\n",
    "                else:\n",
    "                    lp += np.log(self.laplace * 1e-3)\n",
    "            # numeric\n",
    "            for feat in self.num_feats:\n",
    "                mean, var = self.num_stats[(feat, c)]\n",
    "                x = float(x_row[feat])\n",
    "                # gaussian log prob\n",
    "                lp += -0.5 * np.log(2 * np.pi * var) - ((x - mean) ** 2) / (2 * var)\n",
    "            log_probs[c] = lp\n",
    "        return log_probs\n",
    "\n",
    "    def predict(self, X_df):\n",
    "        preds = []\n",
    "        for _, row in X_df.iterrows():\n",
    "            lps = self._log_likelihood(row)\n",
    "            # pick class with max log prob\n",
    "            pred = max(lps.items(), key=lambda kv: kv[1])[0]\n",
    "            preds.append(pred)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81b9df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist_and_train(train_csv='MNIST_data/train.csv', test_csv='MNIST_data/test.csv',\n",
    "                            lr=0.5, epochs=100, batch_size=128, reg=0.0):\n",
    "    print(\"Loading MNIST data...\")\n",
    "    X_train, y_train = load_mnist_csv(train_csv)\n",
    "    X_test, y_test = load_mnist_csv(test_csv)\n",
    "    # normalize to [0,1]\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "    # initialize model\n",
    "    num_features = X_train.shape[1]\n",
    "    num_classes = int(np.max(y_train)) + 1\n",
    "    model = SoftmaxRegression(num_features, num_classes, lr=lr, reg=reg)\n",
    "    print(\"Training softmax regression...\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=True)\n",
    "    print(\"Predicting test set...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_metrics(y_test, y_pred, name=\"MNIST SoftmaxRegression\")\n",
    "    return model, X_test, y_test, y_pred\n",
    "\n",
    "def prepare_bank_and_train(bank_csv=r\"C:\\Users\\dubey\\OneDrive\\Desktop\\Coding\\ML_Lab\\Lab9\\bank-full.csv\", train_indices_file=None, test_indices_file=None,\n",
    "                           numeric_threshold=15):\n",
    "    print(\"Loading bank-full data...\")\n",
    "    df = pd.read_csv(bank_csv, sep=';')\n",
    "    # target column 'y' typically has 'yes'/'no' strings\n",
    "    if 'y' not in df.columns:\n",
    "        raise ValueError(\"Expected column 'y' in bank-full.csv\")\n",
    "    y = df['y'].values\n",
    "    X = df.drop(columns=['y'])\n",
    "\n",
    "    # if index files provided and exist, use them\n",
    "    if train_indices_file and test_indices_file and os.path.exists(train_indices_file) and os.path.exists(test_indices_file):\n",
    "        print(\"Using provided train/test index files.\")\n",
    "        train_idx = np.load(train_indices_file)\n",
    "        test_idx = np.load(test_indices_file)\n",
    "        X_train = X.iloc[train_idx].reset_index(drop=True)\n",
    "        y_train = y[train_idx]\n",
    "        X_test = X.iloc[test_idx].reset_index(drop=True)\n",
    "        y_test = y[test_idx]\n",
    "    else:\n",
    "        # create reproducible stratified split (70/30)\n",
    "        print(\"No split files found, creating reproducible stratified split (70/30) with random_state=42.\")\n",
    "        from sklearn.model_selection import StratifiedShuffleSplit\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
    "        for train_idx, test_idx in splitter.split(X, y):\n",
    "            X_train = X.iloc[train_idx].reset_index(drop=True)\n",
    "            y_train = y[train_idx]\n",
    "            X_test = X.iloc[test_idx].reset_index(drop=True)\n",
    "            y_test = y[test_idx]\n",
    "\n",
    "    numeric_feats = []\n",
    "    categorical_feats = []\n",
    "    for col in X_train.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X_train[col]):\n",
    "            numeric_feats.append(col)\n",
    "        else:\n",
    "            uniques = X_train[col].nunique()\n",
    "            if uniques > numeric_threshold:\n",
    "                try:\n",
    "                    X_train[col].astype(float)\n",
    "                    numeric_feats.append(col)\n",
    "                    continue\n",
    "                except:\n",
    "                    categorical_feats.append(col)\n",
    "            else:\n",
    "                categorical_feats.append(col)\n",
    "\n",
    "    print(\"Numeric features:\", numeric_feats)\n",
    "    print(\"Categorical features:\", categorical_feats)\n",
    "\n",
    "    model = MixedNaiveBayes(categorical_features=categorical_feats, numeric_features=numeric_feats, laplace=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_metrics(y_test, y_pred, name=\"Bank-full MixedNaiveBayes\")\n",
    "    return model, X_test, y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a008dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data...\n",
      "Loaded MNIST_data/train.csv: X.shape=(42000, 784), y.shape=(42000,)\n",
      "Loaded MNIST_data/test.csv: X.shape=(28000, 784), y.shape=(28000,)\n",
      "Training softmax regression...\n",
      "Epoch 1/50 - loss: 0.3812 - acc: 0.8865\n",
      "Epoch 5/50 - loss: 0.2852 - acc: 0.9200\n",
      "Epoch 10/50 - loss: 0.2668 - acc: 0.9257\n",
      "Epoch 15/50 - loss: 0.2766 - acc: 0.9214\n",
      "Epoch 20/50 - loss: 0.2702 - acc: 0.9226\n",
      "Epoch 25/50 - loss: 0.2594 - acc: 0.9265\n",
      "Epoch 30/50 - loss: 0.2522 - acc: 0.9297\n",
      "Epoch 35/50 - loss: 0.2510 - acc: 0.9293\n",
      "Epoch 40/50 - loss: 0.2493 - acc: 0.9288\n",
      "Epoch 45/50 - loss: 0.2823 - acc: 0.9140\n",
      "Epoch 50/50 - loss: 0.2466 - acc: 0.9286\n",
      "Predicting test set...\n",
      "--- MNIST SoftmaxRegression Metrics ---\n",
      "Accuracy: 0.1008\n",
      "Macro Precision: 0.1000\n",
      "Macro Recall: 0.0101\n",
      "Macro F1: 0.0183\n",
      "Per-class (precision, recall, f1):\n",
      "  0: 1.0000, 0.1008, 0.1831\n",
      "  1: 0.0000, 0.0000, 0.0000\n",
      "  2: 0.0000, 0.0000, 0.0000\n",
      "  3: 0.0000, 0.0000, 0.0000\n",
      "  4: 0.0000, 0.0000, 0.0000\n",
      "  5: 0.0000, 0.0000, 0.0000\n",
      "  6: 0.0000, 0.0000, 0.0000\n",
      "  7: 0.0000, 0.0000, 0.0000\n",
      "  8: 0.0000, 0.0000, 0.0000\n",
      "  9: 0.0000, 0.0000, 0.0000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for first test sample: 2\n",
      "Loading bank-full data...\n",
      "No split files found, creating reproducible stratified split (70/30) with random_state=42.\n",
      "Numeric features: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "Categorical features: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "--- Bank-full MixedNaiveBayes Metrics ---\n",
      "Accuracy: 0.8799\n",
      "Macro Precision: 0.7113\n",
      "Macro Recall: 0.7210\n",
      "Macro F1: 0.7160\n",
      "Per-class (precision, recall, f1):\n",
      "  no: 0.9351, 0.9284, 0.9318\n",
      "  yes: 0.4874, 0.5135, 0.5002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_model, X_test_mnist, y_test_mnist, y_pred_mnist = prepare_mnist_and_train(\n",
    "    lr=0.5, epochs=50, batch_size=256, reg=1e-4\n",
    ")\n",
    "img, lab = predict_sample(X_test_mnist[0]*255.0, mnist_model, normalize_input=True, show=True)\n",
    "print(\"Predicted label for first test sample:\", lab)\n",
    "\n",
    "bank_model, X_test_bank, y_test_bank, y_pred_bank = prepare_bank_and_train(\n",
    "    train_indices_file=\"bank_train_indices.npy\",\n",
    "    test_indices_file=\"bank_test_indices.npy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b97ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
